{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d9cfa1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T13:44:04.690234Z",
     "start_time": "2025-01-03T13:44:03.291305Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from skmultilearn.dataset import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import random\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from skmultilearn.dataset import load_from_arff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hamming_loss, accuracy_score, f1_score, precision_score, recall_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sklearn.metrics as metrics\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import itertools\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from skmultilearn.dataset import load_dataset\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pdb\n",
    "import scipy\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from enum import Enum\n",
    "import math\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from util import *\n",
    "from layers import  *\n",
    "from model import *\n",
    "from function import *\n",
    "from collections import deque\n",
    "from scipy.stats import entropy\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c237c19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T13:44:04.702492Z",
     "start_time": "2025-01-03T13:44:04.692020Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "def seed_all(seed): \n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "seed=42\n",
    "seed_all(seed)\n",
    "device = torch.device(\"cuda:0\" ) if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8699f700",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T13:44:05.186350Z",
     "start_time": "2025-01-03T13:44:05.172013Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    def __init__(self,name,X,y):\n",
    "        self.name=name      \n",
    "        self.X_train=X\n",
    "        self.y_train=y\n",
    "        self.configs={} \n",
    "    def getconfig(self):\n",
    "        self.configs['label_matrix']=np.array(self.y_train)\n",
    "        self.configs['num_classes']=self.y_train.shape[1] \n",
    "        self.configs['num_ins']=self.X_train.shape[0] \n",
    "        self.configs['seed']=42 \n",
    "        self.configs['batch_size']=128\n",
    "        self.configs['epoch']=100 \n",
    "        self.configs['lr']=1e-4\n",
    "        self.configs['device']=torch.device(\"cuda:0\" ) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        self.configs['weight']=limb(np.array(self.X_train),np.array(self.y_train))\n",
    "        self.configs['extra_sample']=int(self.X_train.shape[0]*0.1)\n",
    "        self.configs['min_ins_idx']=minority_instance(np.array(self.y_train))\n",
    "        self.configs['minority_label_indices'],_=Labeltype(np.array(self.y_train))\n",
    "        self.configs['weight_list']=calweight(np.array(self.X_train),np.array(self.y_train))\n",
    "        self.configs['card'],_=CardAndDens(np.array(self.X_train),np.array(self.y_train))\n",
    "        #DELA\n",
    "        if self.name=='DELA':\n",
    "            self.configs['in_features']=self.X_train.shape[1] \n",
    "            self.configs['latent_dim']=math.ceil(self.X_train.shape[1]/2)    \n",
    "            self.configs['lr_ratio']=0.8\n",
    "            self.configs['drop_ratio']=0.2\n",
    "            self.configs['tau']=2/3\n",
    "            self.configs['beta']=1e-4\n",
    "            self.configs['out_index']=-1\n",
    "        if self.name=='CLIF':\n",
    "            self.configs['class_emb_size']=self.y_train.shape[1]  \n",
    "            self.configs['input_x_size']=self.X_train.shape[1] \n",
    "            self.configs['num_layers']=2 \n",
    "            self.configs['in_layers']=3 \n",
    "            self.configs['hidden_list']=[math.ceil(self.y_train.shape[1]/2)]  \n",
    "            self.configs['out_index']=0        \n",
    "        if self.name=='PACA':\n",
    "            self.configs['drop_ratio']=0.1\n",
    "            self.configs['latent_dim']=math.ceil(self.X_train.shape[1]/2)\n",
    "            self.configs['in_features']=self.X_train.shape[1] #输入x的维度\n",
    "            self.configs['rand_seed']=self.configs['seed']\n",
    "            self.configs['eps']=1e-8    \n",
    "            self.configs['lr_scheduler']='fix'\n",
    "            self.configs['binary_data']=False\n",
    "            self.configs['weight_decay']=1e-5\n",
    "            self.configs['alpha']=2\n",
    "            self.configs['gamma']=10\n",
    "            self.configs['scheduler_warmup_epoch']=5\n",
    "            self.configs['scheduler_decay_epoch']=10\n",
    "            self.configs['scheduler_decay_rate']=1e-5 \n",
    "            self.configs['out_index']=-2\n",
    "        return self.configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91e67a79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T13:44:05.893244Z",
     "start_time": "2025-01-03T13:44:05.874995Z"
    },
    "code_folding": [
     2,
     13,
     22,
     38,
     47,
     53,
     67,
     69,
     71
    ]
   },
   "outputs": [],
   "source": [
    "seed_all(seed)\n",
    "device = torch.device('cuda')\n",
    "def FeatureSelect(X,p):\n",
    "    if p==1:\n",
    "        return X.toarray(),feature_names\n",
    "    else:\n",
    "        featurecount=int(X.shape[1]*p)\n",
    "        Selectfeatureindex=[x[0] for x in (sorted(enumerate(X.sum(axis=0).tolist()[0]),key=lambda x: x[1],reverse=True))][:featurecount]\n",
    "        Allfeatureindex=[i for i in range(X.shape[1])]\n",
    "        featureindex=[i for i in Allfeatureindex if i not in Selectfeatureindex]\n",
    "        new_x=np.delete(X.toarray(),featureindex,axis=1)\n",
    "        new_featurename=[feature_names[i] for i in Selectfeatureindex] \n",
    "        return new_x,new_featurename\n",
    "def LabelSelect(y):\n",
    "    b=[]\n",
    "    new_labelname=[i for i in label_names]\n",
    "    for i in range(y.shape[1]):\n",
    "        if y[:,i].sum()<=20:\n",
    "            b.append(i)\n",
    "            new_labelname.remove(label_names[i])\n",
    "    new_y=np.delete(y.toarray(),b,axis=1)\n",
    "    return new_y,new_labelname\n",
    "def macro_averaging_auc(Y, P, O):\n",
    "    n = (Y.shape[0] + O.shape[0]) // 2\n",
    "    l = (Y.shape[1] + O.shape[1]) // 2\n",
    "\n",
    "    p = np.zeros(l)\n",
    "    q = np.sum(Y, 0)\n",
    "\n",
    "    zero_column_count = np.sum(q == 0)\n",
    "#     print(f\"all zero for label: {zero_column_count}\")\n",
    "    r, c = np.nonzero(Y)\n",
    "    for i, j in zip(r, c):\n",
    "        p[j] += np.sum((Y[ : , j] < 0.5) * (O[ : , j] <= O[i, j]))\n",
    "\n",
    "    i = (q > 0) * (q < n)\n",
    "\n",
    "    return np.sum(p[i] / (q[i] * (n - q[i]))) / l\n",
    "def hamming_loss(Y, P, O):\n",
    "    n = (Y.shape[0] + P.shape[0]) // 2\n",
    "    l = (Y.shape[1] + P.shape[1]) // 2\n",
    "\n",
    "    s1 = np.sum(Y, 1)\n",
    "    s2 = np.sum(P, 1)\n",
    "    ss = np.sum(Y * P, 1)\n",
    "\n",
    "    return np.sum(s1 + s2 - 2 * ss) / (n * l)\n",
    "def one_error(Y, P, O):\n",
    "    n = (Y.shape[0] + O.shape[0]) // 2\n",
    "\n",
    "    i = np.argmax(O, 1)\n",
    "\n",
    "    return np.sum(1 - Y[range(n), i]) / n\n",
    "def ranking_loss(Y, P, O):\n",
    "    n = (Y.shape[0] + O.shape[0]) // 2\n",
    "    l = (Y.shape[1] + O.shape[1]) // 2\n",
    "\n",
    "    p = np.zeros(n)\n",
    "    q = np.sum(Y, 1)\n",
    "\n",
    "    r, c = np.nonzero(Y)\n",
    "    for i, j in zip(r, c): \n",
    "        p[i] += np.sum((Y[i, : ] < 0.5) * (O[i, : ] >= O[i, j]))\n",
    "\n",
    "    i = (q > 0) * (q < l)\n",
    "\n",
    "    return np.sum(p[i] / (q[i] * (l - q[i]))) / n\n",
    "def micro_f1(Y, P, O):\n",
    "    return f1_score(Y, P, average='micro')\n",
    "def macro_f1(Y, P, O):\n",
    "    return f1_score(Y, P, average='macro')\n",
    "def eval_metrics(mod, metrics, datasets,batch_size,device):\n",
    "    res_dict = {}\n",
    "    mod.eval()\n",
    "    y_true_list = []\n",
    "    y_scores_list = []\n",
    "    test_dataloader = DataLoader(datasets, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    for x, y in test_dataloader:\n",
    "        _, y_pred=mod.predict(x)\n",
    "#         pdb.set_trace()\n",
    "        y_true_list.append(y.cpu().numpy())\n",
    "        y_scores_list.append(y_pred.cpu().numpy())\n",
    "    y_true = np.vstack(y_true_list)\n",
    "    y_prob = np.vstack(y_scores_list)\n",
    "    y_pred = np.round(y_prob).astype(int)\n",
    "    res_dict1 = {metric.__name__: metric(y_true, y_pred,y_prob) for metric in metrics}\n",
    "#         # Calculate metric.\n",
    "#         res_dict1 = {metric.__name__: metric(y_true, y_pred) for metric in metrics[:2]}\n",
    "#         res_dict2 = {metric.__name__: metric(y_true, y_prob) for metric in metrics[2:5]}\n",
    "#         res_dict1.update(res_dict2)\n",
    "#     res_dict[f'dataset_{ix}']=res_dict1\n",
    "#         res_dict[f'dataset_{ix}'] = {metric.__name__: metric(y_true, y_pred) for metric in metrics[:8]}\n",
    "#         res_dict[f'dataset_{ix}'] = {metric.__name__: metric(y_true, y_prob) for metric in metrics[8:9]}\n",
    "    return res_dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1bf6bbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T13:44:06.486697Z",
     "start_time": "2025-01-03T13:44:06.438433Z"
    },
    "code_folding": [
     0,
     16,
     45,
     128,
     190,
     195,
     241,
     287,
     328,
     365,
     376
    ]
   },
   "outputs": [],
   "source": [
    "def Imbalance(y):\n",
    "    countmatrix=[]\n",
    "    for i in range(y.shape[1]):\n",
    "        count0=0\n",
    "        count1=0\n",
    "        for j in range(y.shape[0]):\n",
    "            if y[j,i]==1:\n",
    "                count1+=1\n",
    "            else:\n",
    "                count0+=1\n",
    "        countmatrix.append(count1)\n",
    "    maxcount=max(countmatrix)\n",
    "    ImbalanceRatioMatrix=[maxcount/i for i in countmatrix]\n",
    "    MaxIR=max(ImbalanceRatioMatrix)\n",
    "    MeanIR=sum(ImbalanceRatioMatrix)/len(ImbalanceRatioMatrix)\n",
    "    return ImbalanceRatioMatrix,MeanIR,countmatrix\n",
    "def Labeltype(y):\n",
    "    # 计算不平衡率矩阵和平均不平衡率\n",
    "    ImbalanceRatioMatrix, MeanIR, _ = Imbalance(y)\n",
    "    \n",
    "    # 计算每个标签与平均不平衡率的差值\n",
    "    DifferenceImbalanceRatioMatrix = [i - MeanIR for i in ImbalanceRatioMatrix]\n",
    "    \n",
    "    MinLabelIndex = []  # 存储少数标签的索引\n",
    "    MajLabelIndex = []  # 存储多数标签的索引\n",
    "    count = 0\n",
    "    \n",
    "    # 根据差值将标签分类为少数标签或多数标签\n",
    "    for i in DifferenceImbalanceRatioMatrix:\n",
    "        if i > 0:\n",
    "            MinLabelIndex.append(count)\n",
    "        else:\n",
    "            MajLabelIndex.append(count)\n",
    "        count += 1\n",
    "\n",
    "    # 根据索引获取少数和多数标签的名称\n",
    "    MinLabelName = [label_names[i][0] for i in MinLabelIndex]\n",
    "    MajLabelName = [label_names[i][0] for i in MajLabelIndex]\n",
    "    \n",
    "    # 创建字典，键是标签索引，值是标签名称\n",
    "    MinLabeldic = dict(zip(MinLabelIndex, MinLabelName))\n",
    "    MajLabeldic = dict(zip(MajLabelIndex, MajLabelName))\n",
    "    \n",
    "    return MinLabeldic, MajLabeldic\n",
    "\n",
    "def Neighbor_Based(train_X, train_y, p):\n",
    "    # 定义 LocalType 枚举类\n",
    "    class LocalType(Enum):\n",
    "        Safe = 1\n",
    "        BoderLine = 2\n",
    "        Rare = 3\n",
    "        Outlier = 4\n",
    "        Majority = 0\n",
    "\n",
    "    np.random.seed(21)\n",
    "    n_neighbors = 5\n",
    "    # 计算最近邻\n",
    "    nbs = NearestNeighbors(n_neighbors=n_neighbors + 1, metric='euclidean', algorithm='kd_tree').fit(train_X)\n",
    "    euclidean, indices = nbs.kneighbors(train_X)\n",
    "\n",
    "    # 计算每个标签列中的少数类标签\n",
    "    minlabels = (np.sum(train_y == 1, axis=0) <= np.sum(train_y == 0, axis=0)).astype(int)\n",
    "\n",
    "    # 计算邻居标签一致性 C\n",
    "    C = np.array([\n",
    "        [np.mean(train_y[indices[i, 1:], j] != train_y[i, j]) if train_y[i, j] == minlabels[j] else 0\n",
    "         for j in range(train_y.shape[1])] for i in range(train_y.shape[0])\n",
    "    ])\n",
    "\n",
    "    # 计算样本权重 W\n",
    "    W = np.sum(np.where(C < 1, C / np.sum(C[C < 1], axis=0, keepdims=True), 0), axis=1)\n",
    "\n",
    "    # 过采样样本数\n",
    "    n_sample = int(train_X.shape[0] * p)\n",
    "    new_X = np.zeros((n_sample, train_X.shape[1]))\n",
    "    new_y = np.zeros((n_sample, train_y.shape[1]))\n",
    "\n",
    "    # 确定实例类型\n",
    "    InstanceType = np.zeros_like(C)\n",
    "    thresholds = [(0.3, LocalType.Safe), (0.7, LocalType.BoderLine), (1, LocalType.Rare)]\n",
    "    \n",
    "    for j in range(train_y.shape[1]):\n",
    "        InstanceType[:, j] = np.select(\n",
    "            [C[:, j] < t[0] for t in thresholds] + [C[:, j] >= 1],\n",
    "            [t[1].value for t in thresholds] + [LocalType.Outlier.value]\n",
    "        )\n",
    "\n",
    "    seed_reference_pairs = []\n",
    "    label_mismatches = []\n",
    "\n",
    "    # 采样并生成新数据\n",
    "    SumW = np.sum(W)\n",
    "    for i in range(n_sample):\n",
    "        random_count = np.random.random() * SumW\n",
    "        seed = np.searchsorted(np.cumsum(W), random_count)\n",
    "\n",
    "        reference = np.random.choice(indices[seed, 1:])\n",
    "        ratio = np.random.random()\n",
    "        seed_reference_pairs.append([seed, reference])\n",
    "\n",
    "        # 生成新样本的特征\n",
    "        new_X[i] = train_X[seed] + ratio * (train_X[reference] - train_X[seed])\n",
    "\n",
    "        # 计算新样本的标签\n",
    "        diff = []\n",
    "        for j in range(train_y.shape[1]):\n",
    "            npseed, npreference = train_y[seed, j], train_y[reference, j]\n",
    "            dist1, dist2 = np.linalg.norm(new_X[i] - train_X[seed]), np.linalg.norm(new_X[i] - train_X[reference])\n",
    "            cd = dist1 / (dist1 + dist2)\n",
    "\n",
    "            if npseed == npreference:\n",
    "                new_y[i, j] = npseed\n",
    "            else:\n",
    "                theta = {LocalType.Safe.value: 0.5, LocalType.BoderLine.value: 0.75, LocalType.Rare.value: 1 + 1e-5,\n",
    "                         LocalType.Outlier.value: 0 - 1e-5}.get(InstanceType[seed, j], 0.5)\n",
    "                if InstanceType[seed, j] == LocalType.Majority.value:\n",
    "                    npseed, npreference = npreference, npseed\n",
    "                    cd = 1 - cd\n",
    "                new_y[i, j] = npseed if cd <= theta else npreference\n",
    "\n",
    "                if npseed != npreference:\n",
    "                    diff.append(j)\n",
    "        label_mismatches.append(diff)\n",
    "\n",
    "    combined_X = np.vstack([train_X, new_X])\n",
    "    combined_y = np.vstack([train_y, new_y])\n",
    "\n",
    "    return combined_X, combined_y, seed_reference_pairs, label_mismatches\n",
    "def Random_Copy(df1, df2, p):\n",
    "    SamplesToClone = int(df1.shape[0] * p)\n",
    "    sub_index = list()\n",
    "    ImbalanceRatioMatrix, MeanIR, countmatrix = Imbalance(np.array(df2))\n",
    "    MinLabeldic, _ = Labeltype(np.array(df2))\n",
    "    \n",
    "    # 将 df1 和 df2 初始化为 numpy 数组\n",
    "    ML_ROS_new_X = np.copy(df1)\n",
    "    ML_ROS_target = np.copy(df2)\n",
    "    MinLabelindex = list(MinLabeldic.keys())\n",
    "    tem = ['True'] * len(MinLabelindex)\n",
    "    dic = dict(zip(MinLabelindex, tem))\n",
    "    minlabels = []\n",
    "\n",
    "    # 遍历每个标签列，计算标签1和0的数量\n",
    "    for i in range(df2.shape[1]):\n",
    "        count0 = 0\n",
    "        count1 = 0\n",
    "        for j in range(df2.shape[0]):\n",
    "            if df2[j, i] == 1:\n",
    "                count1 += 1\n",
    "            else:\n",
    "                count0 += 1\n",
    "        if count1 <= count0:\n",
    "            minlabels.append(1)\n",
    "        else:\n",
    "            minlabels.append(0)\n",
    "\n",
    "    while SamplesToClone > 0:\n",
    "        for tail_label_index in MinLabelindex:\n",
    "            if dic[tail_label_index] == 'False':\n",
    "                continue\n",
    "            \n",
    "            # 确保 MinLabeldic 返回整数索引\n",
    "            label_column_index = MinLabeldic[tail_label_index]\n",
    "\n",
    "            new_IRLbl = max(countmatrix) / countmatrix[tail_label_index]\n",
    "            if new_IRLbl <= MeanIR:\n",
    "                dic[tail_label_index] = 'False'\n",
    "                continue\n",
    "\n",
    "            # 使用正确的列索引找到满足条件的子集\n",
    "            sub_index = np.where(df2[:, tail_label_index] == 1)[0]\n",
    "            randomindex = np.random.choice(sub_index)\n",
    "\n",
    "            for index in MinLabelindex:\n",
    "                if df2[randomindex, index] == minlabels[tail_label_index]:\n",
    "                    countmatrix[index] += 1\n",
    "\n",
    "            # 使用 np.vstack 来扩展数组\n",
    "            ML_ROS_new_X = np.vstack([ML_ROS_new_X, df1[randomindex]])\n",
    "            ML_ROS_target = np.vstack([ML_ROS_target, df2[randomindex]])\n",
    "            SamplesToClone -= 1\n",
    "\n",
    "            if SamplesToClone <= 0:\n",
    "                break\n",
    "\n",
    "        if set(dic.values()) == {'False'}:\n",
    "            break\n",
    "\n",
    "    return ML_ROS_new_X, ML_ROS_target\n",
    "    \n",
    "def NN(df1):\n",
    "    nbs=NearestNeighbors(n_neighbors=df1.shape[0],metric='euclidean',algorithm='kd_tree').fit(df1)\n",
    "    euclidean,indices= nbs.kneighbors(df1)\n",
    "    return euclidean,indices\n",
    "\n",
    "def seed_construct(df1, df2, MinLabelindex):\n",
    "    S_s_all_dic = {}\n",
    "    S_c_all_dic = {}\n",
    "    S_s_all = []\n",
    "    S_c_all = []\n",
    "    S_in = []\n",
    "    euclidean, indices = NN(df1)\n",
    "    \n",
    "    for label in range(df2.shape[1]):\n",
    "        maj_index = np.where(df2[:, label] == 0)[0]  # Get indices where df2[:, label] is 0 (majority class)\n",
    "        min_index = np.where(df2[:, label] == 1)[0]  # Get indices where df2[:, label] is 1 (minority class)\n",
    "        \n",
    "        S_s = []\n",
    "        S_c = []\n",
    "        \n",
    "        for i in maj_index:\n",
    "            top_indices = []\n",
    "            for j in indices[i, 1:]:\n",
    "                if df2[j, label] == 1:\n",
    "                    top_indices.append(j)\n",
    "                if len(top_indices) == 3:\n",
    "                    break\n",
    "            S_s = list(set(S_s).union(set(top_indices)))\n",
    "        \n",
    "        for i in S_s:\n",
    "            top_indices = []\n",
    "            for j in indices[i, 1:]:\n",
    "                if df2[j, label] == 0:\n",
    "                    top_indices.append(j)\n",
    "                if len(top_indices) == 3:\n",
    "                    break\n",
    "            S_c = list(set(S_c).union(set(top_indices)))\n",
    "        \n",
    "        if label in MinLabelindex:\n",
    "            S_s_all_dic[label] = S_s\n",
    "            S_s_all = list(set(S_s_all).union(set(S_s)))\n",
    "        \n",
    "        S_c_all_dic[label] = S_c\n",
    "        S_s_tmp = list(set(S_s_all).union(set(S_s)))\n",
    "        S_c_all = list(set(S_c_all).union(set(S_c)))\n",
    "    \n",
    "    set3 = set(S_in)\n",
    "    S_in = list(set3 - set(S_s_tmp + S_s_all))\n",
    "    \n",
    "    return S_s_all, S_c_all, S_in, S_s_all_dic, S_c_all_dic\n",
    "\n",
    "def Get_P_s(df1, df2, MinLabelindex, S_s_all, S_s_all_dic):\n",
    "    euclidean, indices = NN(df1)\n",
    "    I_s = np.zeros((len(S_s_all), len(MinLabelindex)))\n",
    "    \n",
    "    for i in S_s_all:\n",
    "        for tail_label in MinLabelindex:\n",
    "            maj_index = np.where(df2[:, tail_label] == 0)[0]  # Majority class\n",
    "            min_index = np.where(df2[:, tail_label] == 1)[0]  # Minority class\n",
    "            \n",
    "            if i in S_s_all_dic[tail_label]:  \n",
    "                distances_maj = []    \n",
    "                for j in indices[i, 1:]:\n",
    "                    if df2[j, tail_label] == 0:\n",
    "                        distances_maj.append(euclidean[i, j])\n",
    "                    if len(distances_maj) == 5:\n",
    "                        break\n",
    "                distance_sum_maj = sum(distances_maj)\n",
    "                \n",
    "                distances_min = []    \n",
    "                for j in indices[i, 1:]:\n",
    "                    if df2[j, tail_label] == 1:\n",
    "                        distances_min.append(euclidean[i, j])\n",
    "                    if len(distances_min) == 5:\n",
    "                        break\n",
    "                distance_sum_min = sum(distances_min)\n",
    "                \n",
    "                I_s[S_s_all.index(i), MinLabelindex.index(tail_label)] = distance_sum_min / distance_sum_maj\n",
    "    \n",
    "    I_s_norm = (I_s - I_s.min(axis=0)) / (I_s.max(axis=0) - I_s.min(axis=0))\n",
    "    I_s_norm_adjust = np.zeros_like(I_s_norm)\n",
    "    n_k_all = []\n",
    "\n",
    "    for label in S_s_all_dic.keys():\n",
    "        min_index = len(np.where(df2[:, label] == 1)[0])  # Count of minority samples for each label\n",
    "        n_k_all.append(min_index)\n",
    "\n",
    "    for i in range(I_s_norm.shape[0]):\n",
    "        for j in range(I_s_norm.shape[1]):\n",
    "            n_j = len(np.where(df2[:, MinLabelindex[j]] == 1)[0])\n",
    "            I_s_norm_adjust[i, j] = I_s_norm[i, j] / (1 + (min(n_k_all)) / (max(n_k_all) - min(n_k_all)))\n",
    "    \n",
    "    W_s = np.max(I_s_norm_adjust, axis=1)\n",
    "    P_s = W_s / np.sum(W_s)\n",
    "    \n",
    "    return P_s\n",
    "\n",
    "def Get_P_c(df1, df2, MinLabelindex, S_c_all, S_c_all_dic, thc):\n",
    "    I_c = np.zeros((len(S_c_all), df2.shape[1]))\n",
    "    euclidean, indices = NN(df1)\n",
    "    \n",
    "    for i in S_c_all:\n",
    "        for label in range(df2.shape[1]):\n",
    "            maj_index = np.where(df2[:, label] == 0)[0]  # Majority class\n",
    "            min_index = np.where(df2[:, label] == 1)[0]  # Minority class\n",
    "            \n",
    "            if i in S_c_all_dic[label]:\n",
    "                distances_maj = []    \n",
    "                for j in indices[i, 1:]:\n",
    "                    if df2[j, label] == 0:\n",
    "                        distances_maj.append(euclidean[i, j])\n",
    "                    if len(distances_maj) == 5:\n",
    "                        break\n",
    "                distance_sum_maj = sum(distances_maj)\n",
    "                \n",
    "                for j in indices[i, 1:]:\n",
    "                    if df2[j, label] == 1:\n",
    "                        z_indices = j\n",
    "                        break\n",
    "        \n",
    "                distances_min = []    \n",
    "                for j in indices[z_indices, 1:]:\n",
    "                    if df2[j, label] == 1:\n",
    "                        distances_min.append(euclidean[i, j])\n",
    "                    if len(distances_min) == 5:\n",
    "                        break\n",
    "                distance_sum_min = sum(distances_min)  \n",
    "\n",
    "                I_c[S_c_all.index(i), label] = distance_sum_maj / distance_sum_min\n",
    "    \n",
    "    I_c_min = np.min(np.ma.masked_where(I_c == 0, I_c), axis=1)\n",
    "    W_c = np.where(I_c_min < thc, 0, I_c_min)\n",
    "    P_c = W_c / sum(W_c)\n",
    "    W_c -= 1\n",
    "    gen_N_c = sum(W_c)\n",
    "    \n",
    "    return P_c, gen_N_c\n",
    "\n",
    "def SynIns(S, p, euclidean, indices, genins, dfX, dfy):\n",
    "    # Initialize new synthetic samples (new_X) and their corresponding target labels (target)\n",
    "    new_X = np.zeros((int(genins), dfX.shape[1]))\n",
    "    target = np.zeros((int(genins), dfy.shape[1]))\n",
    "\n",
    "    for count in range(int(genins)):\n",
    "        # Choose a seed sample based on probabilities\n",
    "        seed_p = np.random.choice(len(p), p=p)\n",
    "        seed = S[seed_p]\n",
    "        \n",
    "        # Find a reference sample based on the nearest neighbors\n",
    "        reference = np.random.choice(indices[seed, 1:])\n",
    "\n",
    "        # If the reference index is out of bounds, continue to the next iteration\n",
    "        if reference >= dfX.shape[0]:\n",
    "            continue\n",
    "        \n",
    "        # Generate the synthetic sample by interpolating features\n",
    "        for j in range(dfX.shape[1]):\n",
    "            ratio = np.random.random()\n",
    "            if feature_names[j][1] == 'NUMERIC':  # Check if the feature is numeric\n",
    "                new_X[count, j] = dfX[seed, j] + ratio * (dfX[reference, j] - dfX[seed, j])\n",
    "            elif feature_names[j][1] in [['YES', 'NO'], ['0', '1']]:  # For categorical features\n",
    "                rmd = np.random.choice([True, False])\n",
    "                if rmd:\n",
    "                    new_X[count, j] = dfX[seed, j]\n",
    "                else:\n",
    "                    new_X[count, j] = dfX[reference, j]\n",
    "            else:\n",
    "                new_X[count, j] = dfX[seed, j]  # Copy for other types (e.g., string)\n",
    "\n",
    "        # Assign target labels based on the synthetic sample generation\n",
    "        for j in range(dfy.shape[1]):\n",
    "            target[count, j] = dfy[seed, j]\n",
    "\n",
    "    return new_X, target\n",
    "\n",
    "def CalcuNN1(df1, n_neighbor, df_new=None):\n",
    "    nbs = NearestNeighbors(n_neighbors=n_neighbor, metric='euclidean', algorithm='kd_tree').fit(df1)\n",
    "    euclidean, indices = nbs.kneighbors(df1)\n",
    "\n",
    "    if df_new is not None:\n",
    "        euclidean_new, indices_new = nbs.kneighbors(df_new)\n",
    "        euclidean = np.concatenate((euclidean, euclidean_new), axis=0)\n",
    "        indices = np.concatenate((indices, indices_new), axis=0)\n",
    "    \n",
    "    return euclidean, indices  \n",
    "\n",
    "def Same_with_Seed(df1, df2):\n",
    "    new_X = df1.copy()\n",
    "    new_y = df2.copy()\n",
    "\n",
    "    euclidean, indices = CalcuNN1(new_X, 5 + 1, df_new=None)\n",
    "\n",
    "    MinLabeldic, MajLabeldic = Labeltype(new_y)  # Pass as NumPy arrays\n",
    "    ImbalanceRatioMatrix, MeanIR, countmatrix = Imbalance(new_y)  # Pass as NumPy arrays\n",
    "    MinLabelindex = list(MinLabeldic.keys())\n",
    "\n",
    "    S_s_all, S_c_all, S_in, S_s_all_dic, S_c_all_dic = seed_construct(new_X, new_y, MinLabelindex)\n",
    "\n",
    "    gen_N_s = 10 * len(S_s_all)\n",
    "    for i in range(1):\n",
    "        P_s = Get_P_s(df1, df2, MinLabelindex, S_s_all, S_s_all_dic)\n",
    "\n",
    "        df_syn, new_target = SynIns(S_s_all, P_s, euclidean, indices, gen_N_s, new_X, new_y)\n",
    "        euclidean, indices = CalcuNN1(new_X, 5 + 1, df_new=df_syn)\n",
    "\n",
    "        new_X = np.concatenate([new_X, df_syn], axis=0)\n",
    "        new_y = np.concatenate([new_y, new_target], axis=0)\n",
    "\n",
    "        P_c, gen_N_c = Get_P_c(df1, df2, MinLabelindex, S_c_all, S_c_all_dic, 0.01)\n",
    "        gen_N_c = min(int(gen_N_c), int(10 * df1.shape[0] / 1))\n",
    "\n",
    "        if P_c.shape == (0,):\n",
    "            break\n",
    "        if gen_N_c != 0 and gen_N_c > 0:\n",
    "            df_syn, new_target = SynIns(S_c_all, P_c, euclidean, indices, gen_N_c, new_X, new_y)\n",
    "            euclidean, indices = CalcuNN1(new_X, 5 + 1, df_new=df_syn)\n",
    "\n",
    "            new_X = np.concatenate([new_X, df_syn], axis=0)\n",
    "            new_y = np.concatenate([new_y, new_target], axis=0)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    n_all = [len(new_y[new_y[:, label] == 1]) for label in range(new_y.shape[1])]\n",
    "    n_ave = sum(n_all) / len(n_all)\n",
    "    while True:\n",
    "        n_all = [len(new_y[new_y[:, label] == 1]) for label in range(new_y.shape[1])]\n",
    "        gen_in = [max(n_ave - n_all[j], 0) for j in range(len(n_all))]\n",
    "\n",
    "        if max(gen_in) == 0:\n",
    "            break\n",
    "\n",
    "        min_value = min(gen_in)\n",
    "        min_label = gen_in.index(min_value)\n",
    "        label_index = list(np.where(df2[:, min_label] == 1)[0])\n",
    "        relevant = list(set(S_in) & set(label_index))\n",
    "        if len(S_in) == 0:\n",
    "            break\n",
    "        P_in = np.full(len(relevant), 1 / len(relevant))\n",
    "        if P_in != 0:\n",
    "            df_syn, new_target = SynIns(relevant, P_in, euclidean, indices, gen_in[min_label], new_X, new_y)\n",
    "            euclidean, indices = CalcuNN1(new_X, 5 + 1, df_new=df_syn)\n",
    "\n",
    "            new_X = np.concatenate([new_X, df_syn], axis=0)\n",
    "            new_y = np.concatenate([new_y, new_target], axis=0)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return new_X, new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94dbdad1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T13:44:08.165580Z",
     "start_time": "2025-01-03T13:44:08.149122Z"
    },
    "code_folding": [
     0,
     22,
     79,
     87,
     114,
     132
    ]
   },
   "outputs": [],
   "source": [
    "def compute_positive_sample_ratios(y):\n",
    "    \"\"\"\n",
    "    计算每个类的正样本比率.\n",
    "\n",
    "    参数:\n",
    "    - y: 真实标签 (形状: [n, num_labels])\n",
    "\n",
    "    返回:\n",
    "    - rc: 每个类的正样本比率 (形状: [num_labels])\n",
    "    \"\"\"\n",
    "    \n",
    "    n, q = y.shape\n",
    "    \n",
    "    positive_counts = np.sum(y, axis=0)  # 形状: [q]\n",
    "    \n",
    "    total_counts = n * np.ones(q)  # 形状: [q]\n",
    "    \n",
    "    # 计算正样本比率 rc\n",
    "    r_c = positive_counts / total_counts\n",
    "    \n",
    "    return r_c\n",
    "\n",
    "class PLM:\n",
    "    def __init__(self, r_c, lambda_param):\n",
    "        self.r_c = r_c\n",
    "        self.lambda_param = lambda_param\n",
    "\n",
    "    def update_ratios(self, labels, predictions):\n",
    "        divergences_plus = []\n",
    "        divergences_minus = []\n",
    "        for c in range(labels.shape[1]):\n",
    "            # Get indices of positive and negative samples\n",
    "            positive_indices = np.where(labels[:, c] == 1)[0]\n",
    "            negative_indices = np.where(labels[:, c] == 0)[0]    \n",
    "            positive_indices_p = np.where(predictions[:, c] >= 0.5)[0]\n",
    "            negative_indices_p = np.where(predictions[:, c] < 0.5)[0]\n",
    "            \n",
    "            # Define the sets for positive and negative samples\n",
    "            S_c_plus = labels[positive_indices, c]  \n",
    "            S_hat_c_plus = predictions[positive_indices_p, c]  \n",
    "            S_c_minus = labels[negative_indices, c]  \n",
    "            S_hat_c_minus = predictions[negative_indices_p, c] \n",
    "\n",
    "            # Compute probability distributions\n",
    "            P_c_plus_hist, _ = np.histogram(S_c_plus, bins=5, range=(0, 1), density=True)\n",
    "            P_c_minus_hist, _ = np.histogram(S_c_minus, bins=5, range=(0, 1), density=True)     \n",
    "            P_hat_c_plus_hist, _ = np.histogram(S_hat_c_plus, bins=5, range=(0, 1), density=True)\n",
    "            P_hat_c_minus_hist, _ = np.histogram(S_hat_c_minus, bins=5, range=(0, 1), density=True)\n",
    "\n",
    "            # Prevent division by zero\n",
    "            P_c_plus_hist += 1e-10\n",
    "            P_c_minus_hist += 1e-10       \n",
    "            P_hat_c_plus_hist += 1e-10\n",
    "            P_hat_c_minus_hist += 1e-10\n",
    "\n",
    "            # Compute KL divergence\n",
    "            D_c_plus = entropy(P_hat_c_plus_hist, P_c_plus_hist)\n",
    "            D_c_minus = entropy(P_hat_c_minus_hist, P_c_minus_hist)\n",
    "\n",
    "            divergences_plus.append(D_c_plus)\n",
    "            divergences_minus.append(D_c_minus)\n",
    "\n",
    "        # Normalize divergences\n",
    "        mu_plus = np.mean(divergences_plus)\n",
    "        sigma_plus = np.std(divergences_plus)\n",
    "\n",
    "        mu_minus = np.mean(divergences_minus)\n",
    "        sigma_minus = np.std(divergences_minus)\n",
    "\n",
    "        normalized_plus = [(d - mu_plus) / sigma_plus for d in divergences_plus]\n",
    "        normalized_minus = [(d - mu_minus) / sigma_minus for d in divergences_minus]\n",
    "\n",
    "        # Update class ratios\n",
    "        for c in range(len(self.r_c)):\n",
    "            D_c = normalized_plus[c] - normalized_minus[c]\n",
    "            self.r_c[c] *= np.exp(self.lambda_param * D_c)\n",
    "\n",
    "        return self.r_c\n",
    "    \n",
    "def update_H(H, y_pred, ids, max_history_length=5):\n",
    "    y_pred_numpy = y_pred.detach().cpu().numpy()\n",
    "    for i, idx in enumerate(ids):\n",
    "        if idx not in H:\n",
    "            H[idx] = deque(maxlen=max_history_length) \n",
    "        H[idx].append(y_pred_numpy[i])  \n",
    "    return H\n",
    "\n",
    "def update_E(H, E, ids, label_dim):\n",
    "    \"\"\"\n",
    "    Update the E matrix based on the current predictions history in H.\n",
    "    \"\"\"\n",
    "    for idx in ids:\n",
    "        current_predictions_history = H[idx]\n",
    "        \n",
    "        # Check if the history is empty\n",
    "        if len(current_predictions_history) == 0:\n",
    "            # Skip or initialize a default value for E[idx]\n",
    "            E[idx, :] = np.zeros(label_dim)  # You can also set another default value if needed\n",
    "            continue\n",
    "        \n",
    "        last_row_index = len(current_predictions_history) - 1\n",
    "        \n",
    "        for j in range(label_dim):\n",
    "            # Calculate E[idx, j] only if the history has valid predictions\n",
    "            p = current_predictions_history[last_row_index][j]\n",
    "            \n",
    "            # Avoid log(0) issues by adding a small epsilon\n",
    "            epsilon = 1e-10\n",
    "            p = np.clip(p, epsilon, 1 - epsilon)  \n",
    "            \n",
    "            E[idx, j] = -1 / np.log(2) * (p * np.log(p) + (1 - p) * np.log(1 - p))\n",
    "    \n",
    "    return E\n",
    "\n",
    "def update_dataset_label(E, dataset, ids, label_dim,threshold):\n",
    "    # 获取 dataset 中的标签 (y) 并将其转换为 numpy 数组\n",
    "    y = dataset.tensors[1].cpu().numpy()\n",
    "    # 遍历样本，从 start 索引开始\n",
    "    for i in ids:\n",
    "        for j in range(label_dim):  # 遍历每个标签\n",
    "            if E[i, j] >= threshold:  # 判断 E 矩阵的值是否小于等于阈值\n",
    "                # 如果当前标签为 0，则将其更新为 1，反之为 0\n",
    "                y[i, j] = 1 if y[i, j] == 0 else 0\n",
    "\n",
    "    # 将更新后的 y 重新转换为 tensor，并封装回 TensorDataset 中\n",
    "    updated_y = torch.tensor(y, dtype=torch.float, device=dataset.tensors[1].device)\n",
    "    \n",
    "    # 保留原始的输入特征，更新后的标签重新封装为 TensorDataset\n",
    "    updated_dataset = TensorDataset(dataset.tensors[0], updated_y)\n",
    "\n",
    "    return updated_dataset\n",
    "        \n",
    "class CustomDataLoader:\n",
    "    def __init__(self, dataset, net, configs, H, E, warm_epoch, origin_flags):\n",
    "        self.dataset = dataset\n",
    "        self.net = net\n",
    "        self.batch_size = configs['batch_size']\n",
    "        self.original_indices = list(range(len(self.dataset)))  # 保持原始索引\n",
    "        self.dataset_indices = self.original_indices.copy()  # 用于迭代的数据索引\n",
    "        self.origin_flags = origin_flags  # 标记每条数据是否来自原始数据集\n",
    "        self.warm_epoch = warm_epoch\n",
    "        self.H = H\n",
    "        self.E = E\n",
    "        self.current_epoch = 0  # 初始化当前 epoch\n",
    "        self.shuffle = True  # 是否打乱数据\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"迭代返回数据的批次\"\"\"\n",
    "        # 如果需要打乱数据，先打乱 dataset_indices\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.dataset_indices)\n",
    "\n",
    "        # 按批次索引切片遍历数据\n",
    "        for start_idx in range(0, len(self.dataset_indices), self.batch_size):\n",
    "            end_idx = min(start_idx + self.batch_size, len(self.dataset_indices))\n",
    "            batch_indices = self.dataset_indices[start_idx:end_idx]\n",
    "\n",
    "            # 返回原始索引, 数据 (x, y) 和标志位\n",
    "            yield self.get_data_from_indices(batch_indices)\n",
    "\n",
    "    def get_data_from_indices(self, indices):\n",
    "        \"\"\"根据索引返回对应的数据\"\"\"\n",
    "        # 使用原始索引获取数据\n",
    "        x, y = zip(*[self.dataset[self.original_indices[i]] for i in indices])\n",
    "        return indices, torch.stack(x), torch.stack(y)\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        \"\"\"设置当前的 epoch\"\"\"\n",
    "        self.current_epoch = epoch\n",
    "        # 在每个 epoch 开始时打乱数据\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.dataset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "870e4e50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T11:26:20.374046Z",
     "start_time": "2025-01-03T11:26:20.355200Z"
    },
    "code_folding": [
     0,
     92
    ]
   },
   "outputs": [],
   "source": [
    "def training(configs, warm_epoch, r_c, lambda_param, threshold, origin_flags):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    net = DELA(configs).to(device)\n",
    "    num_epochs = configs['epoch']\n",
    "    batch_size = configs['batch_size']\n",
    "    \n",
    "    # 优化器和超参数设置\n",
    "    lr = 1e-4\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=1e-4)\n",
    "    writer = SummaryWriter(comment=f'{dataname}')\n",
    "\n",
    "    # 初始化数据集的H和E\n",
    "    ins_dim = len(origin_flags)\n",
    "    true_count = origin_flags.count(True)\n",
    "    generate_idx = [i for i in range(ins_dim) if true_count <= i <= ins_dim]\n",
    "    \n",
    "    label_dim = configs['num_classes']\n",
    "    sample_indices = list(range(ins_dim))\n",
    "    H = {idx: deque(maxlen=5) for idx in sample_indices}\n",
    "    E = np.zeros((ins_dim, label_dim))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 自定义数据加载器\n",
    "    custom_dataloader = CustomDataLoader(train_dataset, net=net, configs=configs, H=H, E=E, warm_epoch=warm_epoch, origin_flags=origin_flags)\n",
    "    validation_dataset, test_dataset_new = train_test_split(test_dataset, test_size=0.5, random_state=42)\n",
    "    test_dataloader = DataLoader(test_dataset_new, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # 初始化训练状态\n",
    "    best_auc = 0\n",
    "    best_model_state = None\n",
    "    global_step = 0\n",
    "    warmup_steps = warm_epoch * int(ins_dim / batch_size)\n",
    "    plm = PLM(r_c=r_c, lambda_param=lambda_param)\n",
    "    \n",
    "    # 学习率更新函数\n",
    "    def update_learning_rate(optimizer, global_step, warmup_steps, base_lr):\n",
    "        lr = base_lr * (global_step / warmup_steps) if global_step < warmup_steps else base_lr\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    for epoch in range(num_epochs + 1):\n",
    "        net.train()\n",
    "        all_y_pred = []\n",
    "        all_y_true = []\n",
    "        custom_dataloader.set_epoch(epoch)\n",
    "        \n",
    "        # 批次训练\n",
    "        for idx, x, y in custom_dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(x)\n",
    "            \n",
    "            # 损失计算与反向传播\n",
    "            loss_dict = net.loss_function_train(outputs, y, r_c,epoch)\n",
    "            loss = loss_dict['Loss']\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            update_learning_rate(optimizer, global_step, warmup_steps, lr)\n",
    "            \n",
    "            if idx in generate_idx:\n",
    "                custom_dataloader.H = update_H(custom_dataloader.H, outputs[configs['out_index']], idx)\n",
    "            \n",
    "            # 二值化预测结果\n",
    "#             binary_outputs = torch.where(outputs[configs['out_index']] >= 0.5, torch.tensor(1.0, device=device), torch.tensor(0.0, device=device))\n",
    "            all_y_pred.append(outputs[configs['out_index']].detach())\n",
    "            all_y_true.append(y.detach())\n",
    "\n",
    "        # 合并所有批次的数据\n",
    "        all_y_pred = torch.cat(all_y_pred, dim=0)\n",
    "        all_y_true = torch.cat(all_y_true, dim=0)\n",
    "        \n",
    "        # 更新 r_c\n",
    "        r_c = plm.update_ratios(all_y_true.cpu().numpy(), all_y_pred.cpu().numpy())\n",
    "        \n",
    "        \n",
    "        # 更新H、E以及数据集标签       \n",
    "        custom_dataloader.E = update_E(custom_dataloader.H, custom_dataloader.E, generate_idx, label_dim)\n",
    "        custom_dataloader.dataset = update_dataset_label(custom_dataloader.E, custom_dataloader.dataset, generate_idx, label_dim,threshold)\n",
    "        \n",
    "        writer.add_scalar('train/loss', loss.item(), epoch)\n",
    "        \n",
    "        # 评估模型\n",
    "        auc, best_model_state = evaluate_model(net, validation_dataloader, best_auc, best_model_state, epoch)\n",
    "    \n",
    "    # 使用最优模型状态进行测试\n",
    "    net.load_state_dict(best_model_state)\n",
    "    mets = eval_metrics(net, [macro_f1, micro_f1, macro_averaging_auc, ranking_loss, hamming_loss, one_error], test_dataset_new, configs['batch_size'], device)\n",
    "    \n",
    "    return mets\n",
    "def evaluate_model(net, dataloader, best_auc, best_model_state, epoch):\n",
    "    net.eval()\n",
    "    y_true_list = []\n",
    "    y_scores_list = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = net(x)\n",
    "            y_true_list.append(y.cpu().numpy())\n",
    "            y_scores_list.append(outputs[configs['out_index']].cpu().numpy())\n",
    "    \n",
    "    y_true = np.vstack(y_true_list)\n",
    "    y_scores = np.vstack(y_scores_list)\n",
    "    \n",
    "    auc = macro_averaging_auc(y_true, y_scores, y_scores)\n",
    "    \n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        best_model_state = net.state_dict().copy()\n",
    "    \n",
    "    return auc, best_model_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c41c0b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T13:44:43.100230Z",
     "start_time": "2025-01-03T13:44:25.363656Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: scene\n",
      "samplingrate: 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'training' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_34444/2289203454.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# Perform training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mresult_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarm_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mdicts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training' is not defined"
     ]
    }
   ],
   "source": [
    "path_to_arff_files = [\"scene\",\"yeast\", 'enron','cal500']\n",
    "label_counts = [6,14,53,174]\n",
    "select_feature=[1,1,1,1]\n",
    "# sp=[0.1,0.2,0.3,0.5,0.7]\n",
    "sp=[0]\n",
    "path_to_arff_files = [\"scene\",\"yeast\", \"Corel5k\",\"rcv1subset1\",\"rcv1subset2\",\"rcv1subset3\",\"yahoo-Business1\",\"yahoo-Arts1\",'cal500']\n",
    "label_counts = [ 6,14,374,101,101,101,28,25,174]\n",
    "select_feature=[1,1,1,0.02,0.02,0.02,0.05,0.05,1]\n",
    "for idx, dataname in enumerate(path_to_arff_files):\n",
    "    path_to_arff_file = f\"/home/tt/{dataname}.arff\"\n",
    "    # Load data and select features/labels\n",
    "    X, y, feature_names, label_names = load_from_arff(\n",
    "        path_to_arff_file,\n",
    "        label_count=label_counts[idx],\n",
    "        label_location=\"end\",\n",
    "        load_sparse=False,\n",
    "        return_attribute_definitions=True\n",
    "    )\n",
    "    \n",
    "    X, feature_names = FeatureSelect(X, select_feature[idx])  \n",
    "    y, label_names = LabelSelect(y)\n",
    "    print(f\"Processing dataset: {dataname}\")\n",
    "    # Scale the features\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # Set parameters for training\n",
    "    warm_epoch = 5\n",
    "\n",
    "    # K-Fold cross validation setup\n",
    "    k_fold = IterativeStratification(n_splits=5, order=1, random_state=42)\n",
    "    dicts = []\n",
    "    for i in sp:\n",
    "        p =i  # oversampling ratio\n",
    "        print(f\"samplingrate: {p}\")\n",
    "        # Iterate through k-fold splits\n",
    "        for fold_idx, (train_idx, test_idx) in enumerate(k_fold.split(X, y)):\n",
    "            # Compute positive sample ratios for the training set\n",
    "            # Perform sampling on the training set\n",
    "            train_X, train_y = X[train_idx], y[train_idx]\n",
    "#             train_X, train_y, seed_reference_pairs, label_mismatches = train_set_weight_and_oversampling(train_X, train_y, p)\n",
    "#             train_X, train_y = train_set_weight_and_oversampling(train_X, train_y, p)\n",
    "            train_X, train_y = Same_with_Seed(train_X, train_y)\n",
    "            r_c = compute_positive_sample_ratios(train_y)\n",
    "            # Create the TensorDataset for train and test sets\n",
    "            train_dataset = TensorDataset(\n",
    "                torch.tensor(train_X, device=device, dtype=torch.float),\n",
    "                torch.tensor(train_y, device=device, dtype=torch.float)\n",
    "            )\n",
    "\n",
    "            test_dataset = TensorDataset(\n",
    "                torch.tensor(X[test_idx], device=device, dtype=torch.float),\n",
    "                torch.tensor(y[test_idx], device=device, dtype=torch.float)\n",
    "            )\n",
    "\n",
    "            # Create flags for original data size and oversampled data\n",
    "            original_data_size = len(train_idx)\n",
    "            new_data_size = int(original_data_size * p)\n",
    "            origin_flags = [True] * original_data_size + [False] * new_data_size\n",
    "\n",
    "            # Generate configurations for training\n",
    "            configs = CFG('DELA', X[train_idx], y[train_idx]).getconfig()\n",
    "\n",
    "            # Perform training\n",
    "            result_dict = training(configs, warm_epoch, r_c, 0.01, 0.3, origin_flags)\n",
    "            dicts.append(result_dict)\n",
    "\n",
    "        # Calculate averages and standard deviations across folds\n",
    "        averages_and_stds = {}\n",
    "        for key in dicts[0].keys():\n",
    "            values = [d[key] for d in dicts]\n",
    "            averages_and_stds[key] = {\n",
    "                'average': round(np.mean(values), 4),\n",
    "                'std': round(np.std(values), 4)\n",
    "            }\n",
    "\n",
    "        # Print the results for the current dataset\n",
    "        print(f\"Results for {dataname}:\")\n",
    "        print(averages_and_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97ab73bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T10:36:18.410796Z",
     "start_time": "2025-01-03T10:36:18.392319Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def training(configs, warm_epoch, r_c, lambda_param, threshold, origin_flags):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    net = PACA(configs).to(device)\n",
    "    num_epochs = configs['epoch']\n",
    "    batch_size = configs['batch_size']\n",
    "    \n",
    "    # 优化器和超参数设置\n",
    "    lr = 1e-4\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=1e-4)\n",
    "    writer = SummaryWriter(comment=f'{dataname}')\n",
    "\n",
    "    # 初始化数据集的H和E\n",
    "    ins_dim = len(origin_flags)\n",
    "    true_count = origin_flags.count(True)\n",
    "    generate_idx = [i for i in range(ins_dim) if true_count <= i <= ins_dim]\n",
    "    \n",
    "    label_dim = configs['num_classes']\n",
    "    sample_indices = list(range(ins_dim))\n",
    "    H = {idx: deque(maxlen=5) for idx in sample_indices}\n",
    "    E = np.zeros((ins_dim, label_dim))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 自定义数据加载器\n",
    "    custom_dataloader = CustomDataLoader(train_dataset, net=net, configs=configs, H=H, E=E, warm_epoch=warm_epoch, origin_flags=origin_flags)\n",
    "    validation_dataset, test_dataset_new = train_test_split(test_dataset, test_size=0.5, random_state=42)\n",
    "    test_dataloader = DataLoader(test_dataset_new, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # 初始化训练状态\n",
    "    best_auc = 0\n",
    "    best_model_state = None\n",
    "    global_step = 0\n",
    "    warmup_steps = warm_epoch * int(ins_dim / batch_size)\n",
    "    plm = PLM(r_c=r_c, lambda_param=lambda_param)\n",
    "    \n",
    "    # 学习率更新函数\n",
    "    def update_learning_rate(optimizer, global_step, warmup_steps, base_lr):\n",
    "        lr = base_lr * (global_step / warmup_steps) if global_step < warmup_steps else base_lr\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    for epoch in range(num_epochs + 1):\n",
    "        net.train()\n",
    "        all_y_pred = []\n",
    "        all_y_true = []\n",
    "        custom_dataloader.set_epoch(epoch)\n",
    "        \n",
    "        # 批次训练\n",
    "        for idx, x, y in custom_dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(x,y)\n",
    "            \n",
    "            # 损失计算与反向传播\n",
    "            loss_dict = net.loss_function_train(outputs, y, r_c,epoch)\n",
    "            loss = loss_dict['Loss']\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            update_learning_rate(optimizer, global_step, warmup_steps, lr)\n",
    "            \n",
    "            if idx in generate_idx:\n",
    "                custom_dataloader.H = update_H(custom_dataloader.H, outputs[configs['out_index']], idx)\n",
    "            \n",
    "            # 二值化预测结果\n",
    "#             binary_outputs = torch.where(outputs[configs['out_index']] >= 0.5, torch.tensor(1.0, device=device), torch.tensor(0.0, device=device))\n",
    "            all_y_pred.append(outputs[configs['out_index']].detach())\n",
    "            all_y_true.append(y.detach())\n",
    "\n",
    "        # 合并所有批次的数据\n",
    "        all_y_pred = torch.cat(all_y_pred, dim=0)\n",
    "        all_y_true = torch.cat(all_y_true, dim=0)\n",
    "        \n",
    "        # 更新 r_c\n",
    "        r_c = plm.update_ratios(all_y_true.cpu().numpy(), all_y_pred.cpu().numpy())\n",
    "        \n",
    "        \n",
    "        # 更新H、E以及数据集标签       \n",
    "        custom_dataloader.E = update_E(custom_dataloader.H, custom_dataloader.E, generate_idx, label_dim)\n",
    "        custom_dataloader.dataset = update_dataset_label(custom_dataloader.E, custom_dataloader.dataset, generate_idx, label_dim,threshold)\n",
    "        \n",
    "        writer.add_scalar('train/loss', loss.item(), epoch)\n",
    "        \n",
    "        # 评估模型\n",
    "        auc, best_model_state = evaluate_model(net, validation_dataloader, best_auc, best_model_state, epoch)\n",
    "    \n",
    "    # 使用最优模型状态进行测试\n",
    "    net.load_state_dict(best_model_state)\n",
    "    mets = eval_metrics(net, [macro_f1, micro_f1, macro_averaging_auc, ranking_loss, hamming_loss, one_error], test_dataset_new, configs['batch_size'], device)\n",
    "    \n",
    "    return mets\n",
    "def evaluate_model(net, dataloader, best_auc, best_model_state, epoch):\n",
    "    net.eval()\n",
    "    y_true_list = []\n",
    "    y_scores_list = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = net(x,y)\n",
    "            y_true_list.append(y.cpu().numpy())\n",
    "            y_scores_list.append(outputs[configs['out_index']].cpu().numpy())\n",
    "    \n",
    "    y_true = np.vstack(y_true_list)\n",
    "    y_scores = np.vstack(y_scores_list)\n",
    "    \n",
    "    auc = macro_averaging_auc(y_true, y_scores, y_scores)\n",
    "    \n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        best_model_state = net.state_dict().copy()\n",
    "    \n",
    "    return auc, best_model_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533cb725",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-03T10:36:02.456Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: rcv1subset3\n",
      "samplingrate: 0.1\n"
     ]
    }
   ],
   "source": [
    "path_to_arff_files = [\"scene\",\"yeast\", \"Corel5k\",\"rcv1subset1\",\"rcv1subset2\",\"rcv1subset3\",\"yahoo-Business1\",\"yahoo-Arts1\",'cal500','Corel16k','tmc2007-500','Eurlex-sm']\n",
    "label_counts = [ 6,14,374,101,101,101,28,25,174,153,22,201]\n",
    "select_feature=[1,1,1,0.02,0.02,0.02,0.05,0.05,1,1,1,0.1]\n",
    "sp=[0.1,0.2,0.3,0.5,0.7]\n",
    "\n",
    "path_to_arff_files = [\"rcv1subset3\",\"yahoo-Business1\",\"yahoo-Arts1\",'cal500','Corel16k','tmc2007-500','Eurlex-sm']\n",
    "label_counts = [101,28,25,174,153,22,201]\n",
    "select_feature=[0.02,0.05,0.05,1,1,1,0.1]\n",
    "sp=[0.1,0.2,0.3,0.5,0.7]\n",
    "for idx, dataname in enumerate(path_to_arff_files):\n",
    "    path_to_arff_file = f\"/home/tt/{dataname}.arff\"\n",
    "    # Load data and select features/labels\n",
    "    X, y, feature_names, label_names = load_from_arff(\n",
    "        path_to_arff_file,\n",
    "        label_count=label_counts[idx],\n",
    "        label_location=\"end\",\n",
    "        load_sparse=False,\n",
    "        return_attribute_definitions=True\n",
    "    )\n",
    "    \n",
    "    X, feature_names = FeatureSelect(X, select_feature[idx])  \n",
    "    y, label_names = LabelSelect(y)\n",
    "    print(f\"Processing dataset: {dataname}\")\n",
    "    # Scale the features\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # Set parameters for training\n",
    "    warm_epoch = 5\n",
    "\n",
    "    # K-Fold cross validation setup\n",
    "    k_fold = IterativeStratification(n_splits=5, order=1, random_state=42)\n",
    "    dicts = []\n",
    "    for i in sp:\n",
    "        p =i  # oversampling ratio\n",
    "        print(f\"samplingrate: {p}\")\n",
    "        # Iterate through k-fold splits\n",
    "        for fold_idx, (train_idx, test_idx) in enumerate(k_fold.split(X, y)):\n",
    "            # Compute positive sample ratios for the training set\n",
    "            # Perform sampling on the training set\n",
    "            train_X, train_y = X[train_idx], y[train_idx]\n",
    "#             train_X, train_y, seed_reference_pairs, label_mismatches = train_set_weight_and_oversampling(train_X, train_y, p)\n",
    "            train_X, train_y = Random_Copy(train_X, train_y, p)\n",
    "#             train_X, train_y = Same_with_Seed(train_X, train_y)\n",
    "            r_c = compute_positive_sample_ratios(train_y)\n",
    "            # Create the TensorDataset for train and test sets\n",
    "            train_dataset = TensorDataset(\n",
    "                torch.tensor(train_X, device=device, dtype=torch.float),\n",
    "                torch.tensor(train_y, device=device, dtype=torch.float)\n",
    "            )\n",
    "\n",
    "            test_dataset = TensorDataset(\n",
    "                torch.tensor(X[test_idx], device=device, dtype=torch.float),\n",
    "                torch.tensor(y[test_idx], device=device, dtype=torch.float)\n",
    "            )\n",
    "\n",
    "            # Create flags for original data size and oversampled data\n",
    "            original_data_size = len(train_idx)\n",
    "            new_data_size = int(original_data_size * p)\n",
    "            origin_flags = [True] * original_data_size + [False] * new_data_size\n",
    "\n",
    "            # Generate configurations for training\n",
    "            configs = CFG('PACA', X[train_idx], y[train_idx]).getconfig()\n",
    "\n",
    "            # Perform training\n",
    "            result_dict = training(configs, warm_epoch, r_c, 0.01, 0.3, origin_flags)\n",
    "            dicts.append(result_dict)\n",
    "\n",
    "        # Calculate averages and standard deviations across folds\n",
    "        averages_and_stds = {}\n",
    "        for key in dicts[0].keys():\n",
    "            values = [d[key] for d in dicts]\n",
    "            averages_and_stds[key] = {\n",
    "                'average': round(np.mean(values), 4),\n",
    "                'std': round(np.std(values), 4)\n",
    "            }\n",
    "\n",
    "        # Print the results for the current dataset\n",
    "        print(f\"Results for {dataname}:\")\n",
    "        print(averages_and_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede9d581",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
